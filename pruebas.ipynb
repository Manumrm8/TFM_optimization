{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmed_Bandit:\n",
    "    def __init__(self, n_arms, context_size, alpha=0.1, learning_rate=0.01):\n",
    "        self.context_size = context_size  # Cantidad de variables de entrada (d1, d2, d3)\n",
    "        self.n_arms = n_arms  # Posibles acciones\n",
    "        self.alpha = alpha    # Probabilidad de exploración (ε)\n",
    "        self.learning_rate = learning_rate # Tasa de aprendizaje para actualizar los pesos\n",
    "\n",
    "        # Inicializamos los pesos para cada brazo.\n",
    "        # Cada brazo tendrá un vector de pesos de tamaño context_size.\n",
    "        # Además, añadimos un peso para el sesgo (bias) si es necesario,\n",
    "        # lo que implica que el context_size efectivo para los pesos será context_size + 1\n",
    "        # si incluimos un 1 constante en el contexto.\n",
    "        # Para simplificar aquí, asumimos que el contexto ya incluye el término de sesgo si se desea.\n",
    "        self.weights = np.zeros((self.n_arms, self.context_size))\n",
    "\n",
    "    def select_arm(self, context):\n",
    "        if np.random.rand() < self.alpha:  # Exploración: acción aleatoria\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:  # Explotación: mejor acción según el modelo\n",
    "            # Asegurarse de que el contexto sea un array numpy y tenga la forma correcta\n",
    "            context_np = np.array(context).reshape(1, -1) # Convertir a fila vector\n",
    "\n",
    "            # Calcular la recompensa esperada para cada brazo\n",
    "            # La recompensa esperada para cada brazo es el producto punto de su vector de pesos y el contexto.\n",
    "            expected_rewards = np.dot(self.weights, context_np.T).flatten()\n",
    "\n",
    "            print(f'Recompensas esperadas para cada brazo: {expected_rewards}')\n",
    "\n",
    "            exp_rewards = np.exp(expected_rewards - np.max(expected_rewards))\n",
    "            probabilities = exp_rewards / np.sum(exp_rewards)\n",
    "\n",
    "            print(f'Probabilidad de coger cada brazo: {probabilities}')\n",
    "\n",
    "            # Seleccionar un brazo aleatoriamente basado en estas probabilidades\n",
    "            # np.random.choice permite elegir un elemento de una lista\n",
    "            # con probabilidades especificadas.\n",
    "            return np.random.choice(self.n_arms, p=probabilities)\n",
    "        \n",
    "    def decode_action(self, chosen_arm, parameters=['f1', 'f2', 'f3'], k_values=[1, 2, 3, 4, 5]):\n",
    "        param_idx = chosen_arm // len(k_values)\n",
    "        k_idx = chosen_arm % len(k_values)\n",
    "        return parameters[param_idx], k_values[k_idx]\n",
    "\n",
    "    def update(self, chosen_arm, context,reward):\n",
    "        # Asegurarse de que el contexto sea un array numpy\n",
    "        context_np = np.array(context)\n",
    "\n",
    "        print(f'Pesos antes: {self.weights[chosen_arm]}')\n",
    "\n",
    "        # Actualizar los pesos del brazo elegido.\n",
    "        # Multiplicamos la recompensa directamente por el contexto y la tasa de aprendizaje.\n",
    "        # Una recompensa positiva y un contexto dado harán que los pesos se ajusten\n",
    "        # para favorecer ese brazo en contextos similares.\n",
    "        # Una recompensa negativa (o baja) hará que los pesos se ajusten en la dirección opuesta,\n",
    "        # desfavoreciendo ese brazo en contextos similares.\n",
    "        self.weights[chosen_arm] += self.learning_rate * reward * context_np\n",
    "\n",
    "        print(f'Pesos después: {self.weights[chosen_arm]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "context_size=3\n",
    "n_arms = context_size*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAB = MultiArmed_Bandit(n_arms, context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=(20,100,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas esperadas para cada brazo: [0.  0.  0.  0.  0.  0.  0.  0.  4.3 0.  0.  0.  0.  0.  0. ]\n",
      "Probabilidad de coger cada brazo: [0.01140254 0.01140254 0.01140254 0.01140254 0.01140254 0.01140254\n",
      " 0.01140254 0.01140254 0.8403645  0.01140254 0.01140254 0.01140254\n",
      " 0.01140254 0.01140254 0.01140254]\n"
     ]
    }
   ],
   "source": [
    "chosen_arm= MAB.select_arm(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcion, n_veces = MAB.decode_action(chosen_arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahora mejoramos la funcion f2 4 veces, pongamos de ejemplo que el resultado es 400, 230, 27.\n"
     ]
    }
   ],
   "source": [
    "print(f'Ahora mejoramos la funcion {funcion} {n_veces} veces, pongamos de ejemplo que el resultado es 400, 230, 27.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_sol_prev=2.5\n",
    "valor_sol_actual=2.6\n",
    "\n",
    "reward=(valor_sol_actual-valor_sol_prev)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos antes: [0.00666667 0.03333333 0.01666667]\n",
      "Pesos después: [0.01333333 0.06666667 0.03333333]\n"
     ]
    }
   ],
   "source": [
    "MAB.update(chosen_arm, context,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparamos la solución previa con el frente de pareto\n",
    " la solución actual con el frente de pareto, ponderamos la dierencia de cada f entre 0 y 1 (cuanto tendría que mejorar d1 para que fuera solución, cuanto d2 para que fuera solución y cuánto d3)\n",
    "\n",
    "Por ejemplo, solución previa 450, 220, 17. si el 450 fuera 437, sería solución. entonces 1-(437/450). lo mismo con el resto.\n",
    "La nueva, sacamos lo mismo, 400, 230 y 27. si el 400 fuera 397, sería solución. (397/400)+...\n",
    "\n",
    "La recompensa será (valor_solucion_actual-valor_solucion_previa)/3. Si se acerca mucho a soluciones, el valor de recompensa será alto.\n",
    "\n",
    "Si saca una solución exacta del frente de pareto, finalizar y dar un -x?\n",
    "\n",
    "Si saca una solución nueva, recompensar y dar un +x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
